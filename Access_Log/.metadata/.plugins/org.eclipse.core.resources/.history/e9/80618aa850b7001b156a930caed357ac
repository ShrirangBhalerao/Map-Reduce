package Access;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;

import java.io.IOException;

import org.apache.hadoop.*;

public class ALDriver {
	public static void main(String[] args) throws IOException,InterruptedException {
		Configuration c=new Configuration();
		Job job=new Job(c,"AccessLog");

		
		JobClient my_client = new JobClient();
		// Create a configuration object for the job
		JobConf job_conf = new JobConf(ALDriver.class);

		// Set a name of the Job
		job_conf.setJobName("SalePerCountry");

		// Specify data type of output key and value
		job_conf.setOutputKeyClass(Text.class);
		job_conf.setOutputValueClass(IntWritable.class);

		// Specify names of Mapper and Reducer Class
		job_conf.setMapperClass(Access.ALMapper.class);
		job_conf.setReducerClass(Access.ALReducer.class);

		// Specify formats of the data type of Input and output
		job_conf.setInputFormat(TextInputFormat.class);
		job_conf.setOutputFormat(TextOutputFormat.class);

		
		
		FileInputFormat.setInputPaths(job_conf, new Path(args[0]));
		FileOutputFormat.setOutputPath(job_conf, new Path(args[1]));

		my_client.setConf(job_conf);
		try {
			// Run the job 
			JobClient.runJob(job_conf);
		} catch (Exception e) {
			e.printStackTrace();
		}
		
		FileSystem fs=FileSystem.get(job_conf);
		FileStatus[] status=fs.listStatus(new Path("hdfs://localhost:9000"+args[1]));
		FSDataInputStream fd=fs.open(status[1].getPath());
		
		String str=fd.readLine();
		String ip="";
		int max=0;
		
		while(str!=null)
		{
			//split the record
			String parts[]=str.split("	");
			//find most occurred IP
			if(max<Integer.parseInt(parts[1])) {
				
				max=Integer.parseInt(parts[1]);
				ip=parts[0];
			}		
    		str=fd.readLine();
    	}
		
		//print results
		System.out.println("IP address: " + ip);
		System.out.println("No. of occurrences: " + max);   
	}
}
